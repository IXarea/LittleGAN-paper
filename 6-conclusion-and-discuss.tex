\section{Conclusion and Discuss}
\subsection{Conclusion}
In this study, in order to obtain a smaller model with good performance, we have carried out several experiments.
We finally propose a facial image generation and adjustment model and its training method.
We share decoder and encoder among three networks, which are generator, discriminator and adjustor.
In that way, we reduce the model size and the calculation of training.
We adjust facial image in the image space.
It reduces the information loss of original image during the adjustment.
In the training aspect, we use gradient penalty proposed in WGAN-GP\upcite{wgan-gp}.
Most importantly, we innovatively put forward partition training.
We have achieved research targets, which are speeding up convergence and further reducing training calculation.

\subsection{Application}
\subsubsection*{Searching for the target person}
In daily life, in many cases, it is necessary to obtain images of specific person.
For example, convey the portrait information of strangers and obtain images in different states.
In the past, general use are verbal descriptions and drawing sketches.
They have poor real-time performance.
Also, requirements for personnel is high.
The information conveyed is not intuitive enough to be biased.
Our model is small, requires less computing equipment and produces images that are closer to real-world images.
It can be easily deployed in mobile devices to better improve this reality.

\subsubsection*{Expand dataset}
Nowadays, most of machine learning relies on a large amount of data,
    while in reality, there is less data tagged and tagging costs are high.
Using LittleGAN, we can expand dataset.
Machine learning can get more data for training,
    verification and testing.

\subsubsection*{Virtual Image Generation}
On the Internet, for player or non-player characters generation in games and privacy protection,
    individuals or enterprises need personalized avatar generation.
We believe that using this model can meet the above personalized needs at a lower cost.

\subsection{Prospect}

\subsubsection*{More Up-to-date Technologies}

In our pre-study stage, we tried to add a residual layer to the model.
Such an adjustment can make the front network layer of the deep neural network better trained.
But this led to the failure of the model training during the test.
    In that case, we eventually abandoned the improvement.
We will next delve into the reasons and apply more new technologies to improve model performance.


\subsubsection*{More Adjustable Attributes}

Due to the lack of some common attributes in the dataset,
    some attributes are not specific enough.
This makes our model's input is not comprehensive enough to generation or adjustment in any way.
Therefore, we will use the method proposed in StarGAN\upcite{stargan},
    which use multiple datasets for training, to enhance the generalization ability of the model.


\subsubsection*{More HD Images}

Due to dataset limitations, the images used for training have lower resolution in our experiments.
Therefore, the image resolution of LittleGAN output is also lower.
In a later improvement, we will try to use a higher resolution labelled facial dataset like CelebA-HQ\upcite{pix2pixhd},
    to train with a higher performance server for a higher resolution model.


\subsubsection*{Natural Language as Input}

Since we did not find suitable natural language description dataset of facial images at first,
    we have canceled the plan to design model with natural language as input.
Next, we will try to create a natural language description dataset of facial images.
The dataset will be used to design and train a model of facial image generation and adjustment with natural language input,
    further reducing the requirements of LittleGAN for users.
Make it easier for users to generate images.


\subsubsection*{Multi-domain Migration}

LittleGAN is to provide a solution to the needs of facial image generation and adjustment.
It is more suitable for production environment.
The improved method we proposed in the study, like partition training, can also be applied to more fields.
We hope to transfer and apply the results and experience gained in our research to more areas and reduce use-cost of training, deployment and running.


\vspace{4ex}

We have opensourced code of our model, training and testing on Github, \url{https://github.com/ixarea/littlegan}.
We are and will continue to conduct in-depth research to further improve and perfect LittleGAN,
    such as improving the performance, expanding the applicable field and improving the ecosystem.
We believe that with our efforts and the help of the community,
    this research project will enter people's production and life.
    In that case, everyone can have a more convenient life.






