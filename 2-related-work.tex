\section{Related Work}


\subsection{Generative Adversarial Network}

Generative Adversarial Networks (GAN) were proposed by Goodfellow \emph{et al}.\upcite{gan}.
In the original setting, GAN are composed of a generator and a discriminator that are trained with competing goals.
The generator is trained to generate samples towards the true data distribution to fool the discriminator,
    while the discriminator is optimized to distinguish between real samples
    from the true data distribution and fake samples produced by the generator.\upcite{stackgan}
Recently, GAN have shown great potential in simulating complex data distributions.



Our baseline is Deep Convolution GAN(DCGAN).
It uses convolutions to replace fully connected layers for a more stable training process.




\subsection{Image Generation}

Text-to-Image\upcite{text2img} is a method based on DCGAN\upcite{dcgan} for generating images from text description.
They combine embedded description with noise to generate images through convolution.
InfoGAN\upcite{infogan} is able to extract attributes from images,
    enabling conditional generation of images through unsupervised learning.
% BEGAN\upcite{began} use a variational auto-encoder as the discriminator.
% It provides a hyperparameter to adjust the balance between image variety and quality while maintaining training stability.
PGGAN\upcite{pggan} first trains on low-resolution facial image,
    then gradually adds network layers for training to improve the resolution of output images and the stability of the network.

\subsection{Image Translation}
Image translation works with images as both inputs and outputs.
Pix2pix\upcite{pix2pix} uses U-Net network, L1 Loss.
Their generator contains a U-Net combining an encoder with a decoder,
    reducing pressure on each network layer and retaining more original image information.
Their discriminator uses L1 Loss to globally constrain images.
In that case, pix2pix\upcite{pix2pix} can generate images better.
% CycleGAN\upcite{cyclegan} uses two generators and two discriminators to train separately, achieving image conversion between two attribute domains.

Whether it is pix2pix\upcite{pix2pix} or CycleGAN\upcite{cyclegan},
    although they can convert image attributes better,
    it is limited to two definite domains.
If it is required to convert multiple attributes, multiple models are needed.
In that case, model size and computational load are too large to meet actual needs.

\subsection{Facial Image Adjustment}
StarGAN\upcite{stargan} trains across datasets so that the model can learn attributes and features from more datasets.
It can generate images by specifying more attributes.
AttGAN\upcite{attgan} extracts the attributes and noise of images,
    changes attributes and combines noise to generate images.
It can accurately edit facial attributes.
But the process of extracting attributes and noise will cause information loss.
Therefore, some features of original images will be lost in the adjusted images.
TPGAN\upcite{tpgan} uses a variety of Loss to achieve the multi-angle transformation of facial images.
Main method of the research is to build a facial outline and then add more details.



\vspace{3ex}

From the analysis of current situation above,
    we can learn that current facial image generation and adjustment in the face research are generally separate.
Also, various models are required to be used.
The conversion of model is likely to cause certain information loss.
Therefore, we hope to use one model to complete two tasks of facial image generation and adjustment.
In this way, not only network parameters can be shared,
    but also the time and use-cost can be reduced.
During the generation and adjustment process, attribute parameters are unified,
    avoiding extra consumption caused by the conversion between different models.