\section{Related Work}

\subsection{Facial Image Generation}

\subsubsection*{Image Generation}

Text-to-Image\upcite{text2img} proposed image generation.
Based on DCGAN\upcite{dcgan}, they embedded text descriptions into words and combine them with latent map to generate images through convolution.
In StackGAN\upcite{stackgan}, the author also embedded the text description into the text feature map,
    and initially generated the contour of the object,
    and further combined the text feature map to generate higher resolution and more detailed images.
InfoGAN\upcite{infogan} was able to extract attributes from images, enabling conditional generation of images and unsupervised learning.
They also used different discriminators to identify images at different stages, thereby improving the quality of the image.


\subsubsection*{Facial Image Generation}

At present, there are several methods related to the improvement of facial image generation,
    such as BEGAN\upcite{began} and PGGAN\upcite{pggan}.
BEGAN\upcite{began} used variational auto-encoder as the discriminator.
It provides a hyperparameter to instruct the balance between image variety and quality.
It also maintained the stability.
PGGAN\upcite{pggan} first trained low-resolution facial image.
It then gradually increased network layer for training,
therefore improving resolution of output images and stability of the network.



\subsection{Facial Image Adjustment}

\subsubsection*{Image Translation}
Image translation treats the image as outputs.
Pix2pix\upcite{pix2pix} used U-Net network, L1 Loss and PatchGAN classifiers.
In generator, U-Net combined Encoder with Decoder,
    reducing pressure on each network layer and retaining more image information.
In discriminator, it used L1 Loss to globally constrain images.
In partial adjustment of images,
    it used PatchGAN to discriminate image, improving the quality.
Therefore, each layer of convolution combined local characteristics of corresponding layer before convolution
    In that case, pix2pix\upcite{pix2pix} could generate images better.
CycleGAN\upcite{cyclegan} used two generators and two discriminators to train separately achieving image conversion between two attribute domains.
Whether it is pix2pix\upcite{pix2pix} or CycleGAN\upcite{cyclegan},
    although they could convert image attributes better,
    it is limited to two definite domains.
If it is required to convert multiple attributes, multiple models are needed.
In that case, size of the model and amount of calculation are too large to meet actual needs.

\subsubsection*{Facial Image Adjustment}
StarGAN\upcite{stargan} trained across datasets so that the model could learn attributes and features from multiple datasets.
It could generate images by specifying more attributes.
AttGAN\upcite{attgan} extracted the attributes and latent map of images,
    changed attributes and combined latent map to generate images.
It realized adjustment of images.
Pix2pixHD\upcite{pix2pixhd} used different discriminators to discriminate images of different resolutions
    and finally combined features at different resolutions to generate high-resolution facial images.
However, this model is huge and difficult to train.
Apart from that, it is too expensive to deploy and run.
Addtionally, the process of extracting latent map would cause information loss.
So some features of original images will be lost in the adjusted images.
Research on multi-angle transformation of facial images is mainly to generate positive face under the side one.
Main method of the research is to build a facial outline and then add more details.
Taking TPGAN\upcite{tpgan} as an example, TPGAN\upcite{tpgan} used a variety of Loss.
It not only achieved the multi-angle transformation of facial images,
    but also achieved purpose of repairing images and improving the quality of facial images.

\vspace{3ex}

From the above analysis of current situation,
    we can learn that current facial image generation and adjustment in the face research are generally separate.
Also, different models are required to be used.
Moreover, image attribute extraction process needs to go through different models.
The conversion of model is likely to cause certain information loss.
Therefore, we hope to use one model to complete two tasks of facial image generation and adjustment.
In this way, not only network parameters can be shared,
    but also the time and use-cost can be reduced.
During the generation and adjustment process, attribute parameters are unified,
    avoiding extra consumption caused by the conversion between different models.